{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTxayqhVzc7e",
        "outputId": "08aec956-3c10-48bd-af69-a03666fc74ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'IDRISI' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/rsuwaileh/IDRISI.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Jok1WYlVebU",
        "outputId": "76086ba2-9d99-4021-b1c7-52668e5eaea2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn<0.24 in /usr/local/lib/python3.8/dist-packages (0.23.2)\n",
            "Requirement already satisfied: sklearn-crfsuite in /usr/local/lib/python3.8/dist-packages (0.3.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn<0.24) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.8/dist-packages (from scikit-learn<0.24) (1.21.6)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn<0.24) (1.2.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.8/dist-packages (from sklearn-crfsuite) (4.64.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sklearn-crfsuite) (1.15.0)\n",
            "Requirement already satisfied: python-crfsuite>=0.8.3 in /usr/local/lib/python3.8/dist-packages (from sklearn-crfsuite) (0.9.8)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (from sklearn-crfsuite) (0.8.10)\n"
          ]
        }
      ],
      "source": [
        "pip install 'scikit-learn<0.24' sklearn-crfsuite scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_X2Q8Po3EIWe",
        "outputId": "7874db30-c9f5-4ac8-8e65-f4c7bf87dc09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import csv\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.tokenize import word_tokenize\n",
        "import json\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "9gM3T9IxEWuM"
      },
      "outputs": [],
      "source": [
        "def tweet_read(path):\n",
        "    tokens = []\n",
        "    labels = []\n",
        "    t = []\n",
        "    l = []\n",
        "    for token in open(path, encoding='utf-8').read().splitlines(): \n",
        "        if token == '':\n",
        "            tokens.append(t)\n",
        "            labels.append(l)\n",
        "            t = []\n",
        "            l = []\n",
        "            continue\n",
        "        splits = token.split()\n",
        "        t.append(splits[0])\n",
        "        l.append(splits[1])\n",
        "\n",
        "    if len(t) > 0 and len(l) > 0:\n",
        "        tokens.append(t)\n",
        "        labels.append(l)\n",
        "        \n",
        "    return tokens, labels\n",
        "\n",
        "\n",
        "def get_pos(tokens):\n",
        "    pos = []\n",
        "    for i in range(0, len(tokens)):\n",
        "        t_pos = []\n",
        "\n",
        "\n",
        "        for e in nltk.pos_tag(tokens[i]):\n",
        "\n",
        "\n",
        "            for p in e[1].split(\"\\n\"):\n",
        "                t_pos.append(p)\n",
        "        if len(t_pos) != len(tokens[i]):\n",
        "            print(\"mismatch in length\")\n",
        "        pos.append(t_pos)\n",
        "    return pos\n",
        "\n",
        "def conver_to_crf(tokens, pos, labels, out_path):\n",
        "    with open(out_path, mode='w', encoding=\"utf-8\", newline=\"\") as data_file:\n",
        "        writer = csv.writer(data_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "        writer.writerow([\"Sentence #\",\"Word\",\"POS\",\"Tag\"])\n",
        "        for tweet in range(0, len(tokens)):\n",
        "            begin = True\n",
        "            for j in range(0, len(tokens[tweet])):\n",
        "                if begin:\n",
        "                    writer.writerow([\"Sentence: \" + str(tweet+1), tokens[tweet][j], pos[tweet][j], labels[tweet][j]])\n",
        "                    begin = False\n",
        "                else:\n",
        "                    writer.writerow([\"\", tokens[tweet][j], pos[tweet][j], labels[tweet][j]])\n",
        "def convert_to_typeless(labels):\n",
        "    tllabels = []\n",
        "    \n",
        "    for i in range(len(labels)):\n",
        "        tllabels.append([])\n",
        "        for l in labels[i]:\n",
        "            spls = l.split(\"-\")\n",
        "            if len(spls) > 1:\n",
        "                tllabels[i].append(spls[0]+\"-LOC\")\n",
        "            else:\n",
        "                tllabels[i].append(l)\n",
        "    \n",
        "    return tllabels "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "L-tRlkDCeCvY"
      },
      "outputs": [],
      "source": [
        "path = \"/content/\" + \"IDRISI/LMR/data/\"\n",
        "disaster = [\"california_wildfires_2018\", \"canada_wildfires_2016\", \"cyclone_idai_2019\", \"ecuador_earthquake_2016\", \n",
        "          \"greece_wildfires_2018\", \"hurricane_dorian_2019\", \"hurricane_florence_2018\", \"hurricane_harvey_2017\", \n",
        "          \"hurricane_irma_2017\", \"hurricane_maria_2017\", \"hurricane_matthew_2016\", \"italy_earthquake_aug_2016\", \n",
        "          \"kaikoura_earthquake_2016\", \"kerala_floods_2018\", \"maryland_floods_2018\", \"midwestern_us_floods_2019\", \n",
        "          \"pakistan_earthquake_2019\", \"puebla_mexico_earthquake_2017\", \"srilanka_floods_2017\"]\n",
        "\n",
        "for typ in ['typeless']:\n",
        "    for case in ['random']:\n",
        "        for event in disaster:\n",
        "\n",
        "            in_path = path + \"EN/gold-\" + case + \"-bilou/\"+ event \n",
        "            out_path = path + \"EN/gold-\" + case + \"-bilou-crf/\" + typ + \"/\" + event \n",
        "            if not os.path.exists(out_path):\n",
        "                os.makedirs(out_path)\n",
        "\n",
        "            for prt in [\"train\", \"dev\"]:  \n",
        "                prt_in_path = in_path + \"/\" + prt + \".txt\"\n",
        "                tokens, labels = tweet_read(prt_in_path)\n",
        "                pos = get_pos(tokens)\n",
        "                if typ == 'typeless':\n",
        "                    labels = convert_to_typeless(labels)\n",
        "                prt_out_path = out_path + \"/\" + prt + \".csv\"\n",
        "                conver_to_crf(tokens, pos, labels, prt_out_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "Z0hGAhmZIiqq"
      },
      "outputs": [],
      "source": [
        "class SentenceGetter(object):\n",
        "    \n",
        "    def __init__(self, data):\n",
        "        self.n_sent = 1\n",
        "        self.data = data\n",
        "        self.empty = False\n",
        "        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n",
        "                                                           s[\"POS\"].values.tolist(),\n",
        "                                                           s[\"Tag\"].values.tolist())]\n",
        "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
        "        #print('self.grouped')\n",
        "        #print(len(self.grouped))\n",
        "        self.sentences = [s for s in self.grouped]\n",
        "        \n",
        "    def get_next(self):\n",
        "        try:\n",
        "            s = self.data[self.data[\"Sentence #\"] == \"Sentence: {}\".format(self.n_sent)]\n",
        "            self.n_sent += 1\n",
        "            return s[\"Word\"].values.tolist(), s[\"POS\"].values.tolist(), s[\"Tag\"].values.tolist()    \n",
        "        except:\n",
        "            self.empty = True\n",
        "            return None, None, None\n",
        "    \n",
        "        def get_next(self):\n",
        "            try:\n",
        "                s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
        "                self.n_sent += 1\n",
        "                return s\n",
        "            except:\n",
        "                return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "IeSfGRY6HSpe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "# import SentenceGetter as SG\n",
        "from sklearn_crfsuite import CRF\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn_crfsuite.metrics import flat_classification_report\n",
        "\n",
        "import scipy.stats\n",
        "import nltk\n",
        "import sklearn\n",
        "from sklearn.metrics import make_scorer\n",
        "\n",
        "import sklearn_crfsuite\n",
        "from sklearn_crfsuite import scorers\n",
        "from sklearn_crfsuite import metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "-7-CfE-DIUCv"
      },
      "outputs": [],
      "source": [
        "def read_data(file_name):\n",
        "    data = pd.read_csv(file_name, encoding=\"utf-8\")\n",
        "    data = data.fillna(method=\"ffill\")\n",
        "    words = list(set(data[\"Word\"].values))\n",
        "    tags = list(set(data[\"Tag\"].values))\n",
        "\n",
        "    # getter = SG.SentenceGetter(data)\n",
        "    getter = SentenceGetter(data)\n",
        "\n",
        "    sent, pos, tag = getter.get_next()\n",
        "    sentences = getter.sentences\n",
        "    return sentences, words, tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "6hDgBOJAIuUu"
      },
      "outputs": [],
      "source": [
        "# 2. Feature preparation\n",
        "def word2features(sent, i):\n",
        "    word = sent[i][0]\n",
        "    postag = sent[i][1]\n",
        "\n",
        "    features = {\n",
        "        'bias': 1.0,\n",
        "        'word.lower()': word.lower(),\n",
        "        'word[-3:]': word[-3:],\n",
        "        'word[-2:]': word[-2:],\n",
        "        'word.isupper()': word.isupper(),\n",
        "        'word.istitle()': word.istitle(),\n",
        "        'word.isdigit()': word.isdigit(),\n",
        "        'postag': postag,\n",
        "        'postag[:2]': postag[:2],\n",
        "    }\n",
        "    if i > 0:\n",
        "        word1 = sent[i-1][0]\n",
        "        postag1 = sent[i-1][1]\n",
        "        features.update({\n",
        "            '-1:word.lower()': word1.lower(),\n",
        "            '-1:word.istitle()': word1.istitle(),\n",
        "            '-1:word.isupper()': word1.isupper(),\n",
        "            '-1:postag': postag1,\n",
        "            '-1:postag[:2]': postag1[:2],\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True\n",
        "\n",
        "    if i < len(sent) - 1:\n",
        "        word1 = sent[i+1][0]\n",
        "        postag1 = sent[i+1][1]\n",
        "        features.update({\n",
        "            '+1:word.lower()': word1.lower(),\n",
        "            '+1:word.istitle()': word1.istitle(),\n",
        "            '+1:word.isupper()': word1.isupper(),\n",
        "            '+1:postag': postag1,\n",
        "            '+1:postag[:2]': postag1[:2],\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True\n",
        "\n",
        "    return features\n",
        "\n",
        "def sent2features(sent):\n",
        "    return [word2features(sent, i) for i in range(len(sent))]\n",
        "\n",
        "def sent2labels(sent):\n",
        "    return [label for token, postag, label in sent]\n",
        "\n",
        "def sent2tokens(sent):\n",
        "    return [token for token, postag, label in sent]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "OF3xf6LxJLxd"
      },
      "outputs": [],
      "source": [
        "def train_crf(X, y, cv, algorithm, c1, c2, max_iterations):\n",
        "    crf = CRF(\n",
        "        algorithm='lbfgs',\n",
        "        max_iterations=100,\n",
        "        all_possible_transitions=True\n",
        "    )\n",
        "    params_space = {\n",
        "        'c1': scipy.stats.expon(scale=0.5),\n",
        "        'c2': scipy.stats.expon(scale=0.05),\n",
        "    }\n",
        "\n",
        "    labels = ['B-LOC', 'L-LOC', 'U-LOC', 'I-LOC']\n",
        "\n",
        "    # use the same metric for evaluation\n",
        "    f1_scorer = make_scorer(metrics.flat_f1_score,\n",
        "                            average='weighted', labels=labels)\n",
        "\n",
        "    # search\n",
        "    rs = RandomizedSearchCV(crf, params_space,\n",
        "                            cv=3,\n",
        "                            verbose=1,\n",
        "                            n_jobs=-1,\n",
        "                            n_iter=50,\n",
        "                            scoring=f1_scorer)\n",
        "    rs.fit(X, y)\n",
        "    return rs\n",
        "\n",
        "\n",
        "    \n",
        "def run_predict(crf, X_test):   \n",
        "    return crf.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "5dl7Zzc7JO3e"
      },
      "outputs": [],
      "source": [
        "def dump_predictions(output_path, tokens, labels):\n",
        "    writer = open(output_path, 'w', encoding='utf-8', newline=\"\")\n",
        "    \n",
        "    for i in range(len(labels)):\n",
        "        t = [x for x, y, z in tokens[i]]\n",
        "        for j in range(len(labels[i])):\n",
        "            writer.write(t[j] + \"\\t\" + labels[i][j] + \"\\n\")\n",
        "        writer.write(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import chain\n",
        "\n",
        "import nltk\n",
        "import sklearn\n",
        "import scipy.stats\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "import sklearn_crfsuite\n",
        "from sklearn_crfsuite import scorers\n",
        "from sklearn_crfsuite import metrics\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "RHwiYXxJQ0we"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "CPy-YyO2JSGl"
      },
      "outputs": [],
      "source": [
        "# Driver Code\n",
        "def run(event, train_path, test_path, out_path):\n",
        "    \n",
        "    sentences_tr, words_tr, tags_tr = read_data(train_path)\n",
        "    sentences_te, words_te, tags_te = read_data(test_path)\n",
        "\n",
        "    X_train = [sent2features(s) for s in sentences_tr] \n",
        "    y_train = [sent2labels(s) for s in sentences_tr] \n",
        "    X_test = [sent2features(s) for s in sentences_te]\n",
        "    y_test = [sent2labels(s) for s in sentences_te]\n",
        "\n",
        "    crf = train_crf(X_train, y_train, 5, 'lbfgs', 0, 1, 100)\n",
        "\n",
        "    y_pred = crf.predict(X_test)\n",
        "    y_pred = cross_val_predict(estimator=crf, X=X_test, y=y_test, cv=5)\n",
        "\n",
        "    labels = list(crf.classes_)\n",
        "    labels.remove('O')\n",
        "    print(labels)\n",
        "\n",
        "    sorted_labels = sorted(\n",
        "        labels,\n",
        "        key=lambda name: (name[1:], name[0])\n",
        "    )\n",
        "    scores = metrics.flat_classification_report(\n",
        "        y_test, y_pred, labels=sorted_labels, digits=3\n",
        "    )\n",
        "            \n",
        "    dump_predictions(out_path, sentences_te, y_pred)\n",
        "\n",
        "    return X_train, y_train, scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "0gjqyOGYJWut",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "outputId": "0ee9add5-f46b-43da-fab6-dda8d58d18fc"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# path = \"<path to IDRISI data directory>\\\\IDRISI\\\\data\\\\LMR\\\\\"\n",
        "path = \"/content/\" + \"IDRISI/LMR/data/\"\n",
        "\n",
        "events = [\"california_wildfires_2018\", \"canada_wildfires_2016\", \"cyclone_idai_2019\", \"ecuador_earthquake_2016\", \n",
        "          \"greece_wildfires_2018\", \"hurricane_dorian_2019\", \"hurricane_florence_2018\", \"hurricane_harvey_2017\", \n",
        "          \"hurricane_irma_2017\", \"hurricane_maria_2017\", \"hurricane_matthew_2016\", \"italy_earthquake_aug_2016\", \n",
        "          \"kaikoura_earthquake_2016\", \"kerala_floods_2018\", \"maryland_floods_2018\", \"midwestern_us_floods_2019\", \n",
        "          \"pakistan_earthquake_2019\", \"puebla_mexico_earthquake_2017\", \"srilanka_floods_2017\"]\n",
        "\n",
        "  \n",
        "for typ in ['typeless']:# 'typefull', \n",
        "    for case in ['random']: # , 'timebased'\n",
        "        for event in events:\n",
        "            in_path = path + \"EN/gold-\" + case + \"-bilou-crf/\" + typ + \"/\" + event \n",
        "            train_path = in_path + \"/train.csv\"\n",
        "            test_path = in_path + \"/dev.csv\"\n",
        "            out_path = path + \"EN/gold-\" + case + \"-bilou-crf/\" + typ + \"/\" + event + \"-predictions.txt\"\n",
        "            run(event, train_path, test_path, out_path) \n",
        "\n",
        "\n",
        "\n",
        "            X_train, y_train, scores = run(event, train_path, test_path, out_path)  \n",
        "            break # Remove this"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "print(scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzO_UzI0NAnu",
        "outputId": "b1df7fb4-f49c-42c8-ab4c-336dfe8927ac"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC      0.727     0.421     0.533        19\n",
            "       I-LOC      0.000     0.000     0.000         1\n",
            "       L-LOC      0.727     0.421     0.533        19\n",
            "       U-LOC      0.902     0.783     0.838       106\n",
            "\n",
            "   micro avg      0.868     0.683     0.764       145\n",
            "   macro avg      0.589     0.406     0.476       145\n",
            "weighted avg      0.850     0.683     0.753       145\n",
            "\n",
            "CPU times: user 955 µs, sys: 0 ns, total: 955 µs\n",
            "Wall time: 1.07 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kk7ZSOO7dFYk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}